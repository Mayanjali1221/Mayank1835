{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attrition Analytics - Exploratory Analysis & Predictive Modeling\n",
    " \n",
    "Import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "#using the seaborn style for graphs\n",
    "plt.style.use(\"seaborn\")\n",
    "## Read the dataset\n",
    "employee_data = pd.read_excel(\"Attrition.xlsx\")\n",
    "employee_data.head()\n",
    "Age\tAttrition\tBusinessTravel\tDailyRate\tDepartment\tDistanceFromHome\tEducation\tEducationField\tEmployeeCount\tEmployeeNumber\t...\tRelationshipSatisfaction\tStandardHours\tStockOptionLevel\tTotalWorkingYears\tTrainingTimesLastYear\tWorkLifeBalance\tYearsAtCompany\tYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\n",
    "0\t41\tYes\tTravel_Rarely\t1102\tSales\t1\t2\tLife Sciences\t1\t1\t...\t1\t80\t0\t8\t0\t1\t6\t4\t0\t5\n",
    "1\t49\tNo\tTravel_Frequently\t279\tResearch & Development\t8\t1\tLife Sciences\t1\t2\t...\t4\t80\t1\t10\t3\t3\t10\t7\t1\t7\n",
    "2\t37\tYes\tTravel_Rarely\t1373\tResearch & Development\t2\t2\tOther\t1\t4\t...\t2\t80\t0\t7\t3\t3\t0\t0\t0\t0\n",
    "3\t33\tNo\tTravel_Frequently\t1392\tResearch & Development\t3\t4\tLife Sciences\t1\t5\t...\t3\t80\t0\t8\t3\t3\t8\t7\t3\t0\n",
    "4\t27\tNo\tTravel_Rarely\t591\tResearch & Development\t2\t1\tMedical\t1\t7\t...\t4\t80\t1\t6\t3\t3\t2\t2\t2\t2\n",
    "5 rows × 35 columns\n",
    "\n",
    " \n",
    "##looking for any missing values\n",
    "\n",
    "employee_data.isnull().sum()\n",
    "Age                         0\n",
    "Attrition                   0\n",
    "BusinessTravel              0\n",
    "DailyRate                   0\n",
    "Department                  0\n",
    "DistanceFromHome            0\n",
    "Education                   0\n",
    "EducationField              0\n",
    "EmployeeCount               0\n",
    "EmployeeNumber              0\n",
    "EnvironmentSatisfaction     0\n",
    "Gender                      0\n",
    "HourlyRate                  0\n",
    "JobInvolvement              0\n",
    "JobLevel                    0\n",
    "JobRole                     0\n",
    "JobSatisfaction             0\n",
    "MaritalStatus               0\n",
    "MonthlyIncome               0\n",
    "MonthlyRate                 0\n",
    "NumCompaniesWorked          0\n",
    "Over18                      0\n",
    "OverTime                    0\n",
    "PercentSalaryHike           0\n",
    "PerformanceRating           0\n",
    "RelationshipSatisfaction    0\n",
    "StandardHours               0\n",
    "StockOptionLevel            0\n",
    "TotalWorkingYears           0\n",
    "TrainingTimesLastYear       0\n",
    "WorkLifeBalance             0\n",
    "YearsAtCompany              0\n",
    "YearsInCurrentRole          0\n",
    "YearsSinceLastPromotion     0\n",
    "YearsWithCurrManager        0\n",
    "dtype: int64\n",
    "employee_data.info()\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 1470 entries, 0 to 1469\n",
    "Data columns (total 35 columns):\n",
    "Age                         1470 non-null int64\n",
    "Attrition                   1470 non-null object\n",
    "BusinessTravel              1470 non-null object\n",
    "DailyRate                   1470 non-null int64\n",
    "Department                  1470 non-null object\n",
    "DistanceFromHome            1470 non-null int64\n",
    "Education                   1470 non-null int64\n",
    "EducationField              1470 non-null object\n",
    "EmployeeCount               1470 non-null int64\n",
    "EmployeeNumber              1470 non-null int64\n",
    "EnvironmentSatisfaction     1470 non-null int64\n",
    "Gender                      1470 non-null object\n",
    "HourlyRate                  1470 non-null int64\n",
    "JobInvolvement              1470 non-null int64\n",
    "JobLevel                    1470 non-null int64\n",
    "JobRole                     1470 non-null object\n",
    "JobSatisfaction             1470 non-null int64\n",
    "MaritalStatus               1470 non-null object\n",
    "MonthlyIncome               1470 non-null int64\n",
    "MonthlyRate                 1470 non-null int64\n",
    "NumCompaniesWorked          1470 non-null int64\n",
    "Over18                      1470 non-null object\n",
    "OverTime                    1470 non-null object\n",
    "PercentSalaryHike           1470 non-null int64\n",
    "PerformanceRating           1470 non-null int64\n",
    "RelationshipSatisfaction    1470 non-null int64\n",
    "StandardHours               1470 non-null int64\n",
    "StockOptionLevel            1470 non-null int64\n",
    "TotalWorkingYears           1470 non-null int64\n",
    "TrainingTimesLastYear       1470 non-null int64\n",
    "WorkLifeBalance             1470 non-null int64\n",
    "YearsAtCompany              1470 non-null int64\n",
    "YearsInCurrentRole          1470 non-null int64\n",
    "YearsSinceLastPromotion     1470 non-null int64\n",
    "YearsWithCurrManager        1470 non-null int64\n",
    "dtypes: int64(26), object(9)\n",
    "memory usage: 402.0+ KB\n",
    " \n",
    "Exploratory Data Analysis\n",
    "## basic descriptive statistics\n",
    "employee_data.describe()\n",
    "Age\tDailyRate\tDistanceFromHome\tEducation\tEmployeeCount\tEmployeeNumber\tEnvironmentSatisfaction\tHourlyRate\tJobInvolvement\tJobLevel\t...\tRelationshipSatisfaction\tStandardHours\tStockOptionLevel\tTotalWorkingYears\tTrainingTimesLastYear\tWorkLifeBalance\tYearsAtCompany\tYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\n",
    "count\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t1470.0\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t...\t1470.000000\t1470.0\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t1470.000000\t1470.000000\n",
    "mean\t36.923810\t802.485714\t9.192517\t2.912925\t1.0\t1024.865306\t2.721769\t65.891156\t2.729932\t2.063946\t...\t2.712245\t80.0\t0.793878\t11.279592\t2.799320\t2.761224\t7.008163\t4.229252\t2.187755\t4.123129\n",
    "std\t9.135373\t403.509100\t8.106864\t1.024165\t0.0\t602.024335\t1.093082\t20.329428\t0.711561\t1.106940\t...\t1.081209\t0.0\t0.852077\t7.780782\t1.289271\t0.706476\t6.126525\t3.623137\t3.222430\t3.568136\n",
    "min\t18.000000\t102.000000\t1.000000\t1.000000\t1.0\t1.000000\t1.000000\t30.000000\t1.000000\t1.000000\t...\t1.000000\t80.0\t0.000000\t0.000000\t0.000000\t1.000000\t0.000000\t0.000000\t0.000000\t0.000000\n",
    "25%\t30.000000\t465.000000\t2.000000\t2.000000\t1.0\t491.250000\t2.000000\t48.000000\t2.000000\t1.000000\t...\t2.000000\t80.0\t0.000000\t6.000000\t2.000000\t2.000000\t3.000000\t2.000000\t0.000000\t2.000000\n",
    "50%\t36.000000\t802.000000\t7.000000\t3.000000\t1.0\t1020.500000\t3.000000\t66.000000\t3.000000\t2.000000\t...\t3.000000\t80.0\t1.000000\t10.000000\t3.000000\t3.000000\t5.000000\t3.000000\t1.000000\t3.000000\n",
    "75%\t43.000000\t1157.000000\t14.000000\t4.000000\t1.0\t1555.750000\t4.000000\t83.750000\t3.000000\t3.000000\t...\t4.000000\t80.0\t1.000000\t15.000000\t3.000000\t3.000000\t9.000000\t7.000000\t3.000000\t7.000000\n",
    "max\t60.000000\t1499.000000\t29.000000\t5.000000\t1.0\t2068.000000\t4.000000\t100.000000\t4.000000\t5.000000\t...\t4.000000\t80.0\t3.000000\t40.000000\t6.000000\t4.000000\t40.000000\t18.000000\t15.000000\t17.000000\n",
    "8 rows × 26 columns\n",
    "\n",
    "#Mapping the attrition 1 - yes and 0 - no in the new column\n",
    "\n",
    "employee_data[\"left\"] = np.where(employee_data[\"Attrition\"] == \"Yes\",1,0)\n",
    "employee_data.head()\n",
    "Age\tAttrition\tBusinessTravel\tDailyRate\tDepartment\tDistanceFromHome\tEducation\tEducationField\tEmployeeCount\tEmployeeNumber\t...\tStandardHours\tStockOptionLevel\tTotalWorkingYears\tTrainingTimesLastYear\tWorkLifeBalance\tYearsAtCompany\tYearsInCurrentRole\tYearsSinceLastPromotion\tYearsWithCurrManager\tleft\n",
    "0\t41\tYes\tTravel_Rarely\t1102\tSales\t1\t2\tLife Sciences\t1\t1\t...\t80\t0\t8\t0\t1\t6\t4\t0\t5\t1\n",
    "1\t49\tNo\tTravel_Frequently\t279\tResearch & Development\t8\t1\tLife Sciences\t1\t2\t...\t80\t1\t10\t3\t3\t10\t7\t1\t7\t0\n",
    "2\t37\tYes\tTravel_Rarely\t1373\tResearch & Development\t2\t2\tOther\t1\t4\t...\t80\t0\t7\t3\t3\t0\t0\t0\t0\t1\n",
    "3\t33\tNo\tTravel_Frequently\t1392\tResearch & Development\t3\t4\tLife Sciences\t1\t5\t...\t80\t0\t8\t3\t3\t8\t7\t3\t0\t0\n",
    "4\t27\tNo\tTravel_Rarely\t591\tResearch & Development\t2\t1\tMedical\t1\t7\t...\t80\t1\t6\t3\t3\t2\t2\t2\t2\t0\n",
    "5 rows × 36 columns\n",
    "\n",
    "#supressing all the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "Remove not usefull features\n",
    "def NumericalVariables_targetPlots(df,segment_by,target_var = \"Attrition\"):\n",
    "    \"\"\"A function for plotting the distribution of numerical variables and its effect on attrition\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols= 2, figsize = (14,6))    \n",
    "\n",
    "    #boxplot for comparison\n",
    "    sns.boxplot(x = target_var, y = segment_by, data=df, ax=ax[0])\n",
    "    ax[0].set_title(\"Comparision of \" + segment_by + \" vs \" + target_var)\n",
    "    \n",
    "    #distribution plot\n",
    "    ax[1].set_title(\"Distribution of \"+segment_by)\n",
    "    ax[1].set_ylabel(\"Frequency\")\n",
    "    sns.distplot(a = df[segment_by], ax=ax[1], kde=False)\n",
    "    \n",
    "    plt.show()\n",
    "def CategoricalVariables_targetPlots(df, segment_by,invert_axis = False, target_var = \"left\"):\n",
    "    \n",
    "    \"\"\"A function for Plotting the effect of variables(categorical data) on attrition \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(ncols= 2, figsize = (14,6))\n",
    "    \n",
    "    #countplot for distribution along with target variable\n",
    "    #invert axis variable helps to inter change the axis so that names of categories doesn't overlap\n",
    "    if invert_axis == False:\n",
    "        sns.countplot(x = segment_by, data=df,hue=\"Attrition\",ax=ax[0])\n",
    "    else:\n",
    "        sns.countplot(y = segment_by, data=df,hue=\"Attrition\",ax=ax[0])\n",
    "        \n",
    "    ax[0].set_title(\"Comparision of \" + segment_by + \" vs \" + \"Attrition\")\n",
    "    \n",
    "    #plot the effect of variable on attrition\n",
    "    if invert_axis == False:\n",
    "        sns.barplot(x = segment_by, y = target_var ,data=df,ci=None)\n",
    "    else:\n",
    "        sns.barplot(y = segment_by, x = target_var ,data=df,ci=None)\n",
    "        \n",
    "    ax[1].set_title(\"Attrition rate by {}\".format(segment_by))\n",
    "    ax[1].set_ylabel(\"Average(Attrition)\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "Analyizing the variables\n",
    "Numerical Variables\n",
    "Age\n",
    "# we are checking the distribution of employee age and its related to attrition or not\n",
    "\n",
    "NumericalVariables_targetPlots(employee_data,segment_by=\"Age\")\n",
    "\n",
    "We found that median age of employee's in the company is 30 - 40 Yrs. Minimum age is 18 Yrs and Maximum age is 60 Yrs.\n",
    "From the Age Comparision boxplot, majority of people who left the company are below 40 Yrs and among the people who didn't left the company are of age 32 to 40 years\n",
    "Daily Rate & Montly Income & HourlyRate\n",
    "#Analyzing the daily wage rate vs employee left the company or not\n",
    "\n",
    "NumericalVariables_targetPlots(employee_data,\"DailyRate\")\n",
    "\n",
    "NumericalVariables_targetPlots(employee_data,\"MonthlyIncome\")\n",
    "\n",
    "Employee's working with lower daily rates are more prone to leave the company than compared to the employee's working with higher rates. The same trend is resonated with monthly income too.\n",
    "Hourly Rate\n",
    "\n",
    "NumericalVariables_targetPlots(employee_data,\"HourlyRate\")\n",
    "\n",
    "From plot we have seen that there is no significant difference in the hourly rate and attrition. Therefore hourly rate is considered as not signifcant to attrition\n",
    "PercentSalaryHike\n",
    "NumericalVariables_targetPlots(employee_data,\"PercentSalaryHike\")\n",
    "\n",
    "Majority (60% of total strength) of employee's receive 16% salary hike in the company, employee's who received less salary hike have left the company.\n",
    "Total Working years\n",
    "NumericalVariables_targetPlots(employee_data,\"TotalWorkingYears\")\n",
    "\n",
    "sns.lmplot(x = \"TotalWorkingYears\", y = \"PercentSalaryHike\", data=employee_data,fit_reg=False,hue=\"Attrition\",size=6,\n",
    "           aspect=1.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Employee's with less working years have received 25% Salary hike when they switch to another company, but there is no linear relationship between working years and salary hike.\n",
    "Attrition is not seen amomg the employee's having more than 20 years of experience if their salary hike is more than 20%, even if the salary hike is below 20% attrition rate among the employee's is very low.\n",
    "Employee's with lesser years of experience are prone to leave the company in search of better pay, irrespective of salary hike\n",
    " \n",
    " \n",
    "Distance From Home\n",
    "NumericalVariables_targetPlots(employee_data,\"DistanceFromHome\")\n",
    "\n",
    "There is a higher number of people who reside near to offices and hence the attrition levels are lower for distance less than 10. With increase in distance from home, attrition rate also increases\n",
    " \n",
    "Analyizing the variables\n",
    "Categorical Variables\n",
    "Job Involvement\n",
    "#cross tabulation between attrition and JobInvolvement\n",
    "pd.crosstab(employee_data.JobInvolvement,employee_data.Attrition)\n",
    "Attrition\tNo\tYes\n",
    "JobInvolvement\t\t\n",
    "1\t55\t28\n",
    "2\t304\t71\n",
    "3\t743\t125\n",
    "4\t131\t13\n",
    "#calculating the percentage of people having different job involvement rate\n",
    "round(employee_data.JobInvolvement.value_counts()/employee_data.shape[0] * 100,2)\n",
    "3    59.05\n",
    "2    25.51\n",
    "4     9.80\n",
    "1     5.65\n",
    "Name: JobInvolvement, dtype: float64\n",
    "CategoricalVariables_targetPlots(employee_data,\"JobInvolvement\")\n",
    "\n",
    "In the total data set, 59% have high job involvement whereas 25% have medium involvement rate\n",
    "From above plot we can observe that round 50% of people in low job involvement (level 1 & 2) have left the company.\n",
    "Even the people who have high job involmenent have higher attrition rate around 15% in that category have left company\n",
    "JobSatisfaction\n",
    "CategoricalVariables_targetPlots(employee_data,\"JobSatisfaction\")\n",
    "\n",
    "As expected, people with low satisfaction have left the company around 23% in that category. what surprising is out of the people who rated medium and high job satisfaction around 32% has left the company. There should be some other factor which triggers their exit from the company\n",
    "\n",
    " \n",
    "Performance Rating\n",
    "#checking the number of categories under performance rating\n",
    "employee_data.PerformanceRating.value_counts()\n",
    "3    1244\n",
    "4     226\n",
    "Name: PerformanceRating, dtype: int64\n",
    "#calculate the percentage of performance rating per category in the whole dataset\n",
    "round(employee_data.PerformanceRating.value_counts()/employee_data.shape[0] * 100,2)\n",
    "3    84.63\n",
    "4    15.37\n",
    "Name: PerformanceRating, dtype: float64\n",
    "Around 85% of people in the company rated as Excellent and remaining 15% rated as Outstanding\n",
    "\n",
    "CategoricalVariables_targetPlots(employee_data,\"PerformanceRating\")\n",
    "\n",
    "Contrary to normal belief that employee's having higher rating will not leave the company. It may be seen that there is no significant difference between the performance rating and Attrition Rate.\n",
    "\n",
    " \n",
    "RelationshipSatisfaction\n",
    "#percentage of each relationship satisfaction category across the data\n",
    "round(employee_data.RelationshipSatisfaction.value_counts()/employee_data.shape[0],2)\n",
    "3    0.31\n",
    "4    0.29\n",
    "2    0.21\n",
    "1    0.19\n",
    "Name: RelationshipSatisfaction, dtype: float64\n",
    "CategoricalVariables_targetPlots(employee_data,\"RelationshipSatisfaction\")\n",
    "\n",
    "In this too, we found that almost 30% of employees with high and very high RelationshipSatisfaction have left the company. Here also there is no visible trend among the relationshipsatisfaction and attrition rate\n",
    "\n",
    "WorkLifeBalance\n",
    "#percentage of worklife balance rating across the company data\n",
    "round(employee_data.WorkLifeBalance.value_counts()/employee_data.shape[0],2)\n",
    "3    0.61\n",
    "2    0.23\n",
    "4    0.10\n",
    "1    0.05\n",
    "Name: WorkLifeBalance, dtype: float64\n",
    "More than 60% of the employee's rated that they have Better worklife balance and 10% rated for Best worklife balance\n",
    "\n",
    "CategoricalVariables_targetPlots(employee_data,\"WorkLifeBalance\")\n",
    "\n",
    "As expected more than 30% of the people who rated as Bad WorkLifeBalance have left the company and around 15% of the people who rated for Best WorkLifeBalance also left the company\n",
    "CategoricalVariables_targetPlots(employee_data,\"OverTime\")\n",
    "\n",
    "More than 30% of employee's who worked overtime has left the company, where as 90% of employee's who have not experienced overtime has not left the company. Therefore overtime is a strong indicator of attrition\n",
    "\n",
    "BusinessTravel\n",
    "CategoricalVariables_targetPlots(employee_data,segment_by=\"BusinessTravel\")\n",
    "\n",
    "There are more people who travel rarely compared to people who travel frequently. In case of people who travel Frequently around 25% of people have left the company and in other cases attrition rate doesn't vary significantly on travel\n",
    " \n",
    "Department\n",
    "employee_data.Department.value_counts()\n",
    "Research & Development    961\n",
    "Sales                     446\n",
    "Human Resources            63\n",
    "Name: Department, dtype: int64\n",
    "CategoricalVariables_targetPlots(employee_data,segment_by=\"Department\")\n",
    "\n",
    "On comparing departmentwise,we can conclude that HR has seen only a marginal high in turnover rates whereas the numbers are significant in sales department with turnover rates of 39 %. The attrition levels are not appreciable in R & D where 67 % have recorded no attrition.\n",
    "Sales has seen higher attrition levels about 20.6% followed by HR around 18%\n",
    "EducationField\n",
    "employee_data.EducationField.value_counts()\n",
    "Life Sciences       606\n",
    "Medical             464\n",
    "Marketing           159\n",
    "Technical Degree    132\n",
    "Other                82\n",
    "Human Resources      27\n",
    "Name: EducationField, dtype: int64\n",
    "CategoricalVariables_targetPlots(employee_data,\"EducationField\",invert_axis=True)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.barplot(y = \"EducationField\", x = \"left\", hue=\"Education\", data=employee_data,ci=None)\n",
    "plt.show()\n",
    "\n",
    "There are more people with a Life sciences followed by medical and marketing\n",
    "Employee's in the EducationField of Human Resources and Technical Degree have highest attrition levels around 26% and 23% respectively\n",
    "When compared with Education level, we have observed that employees in the highest level of education in there field of study have left the company. We can conclude that EducationField is a strong indicator of attrition\n",
    "EnvironmentSatisfaction\n",
    "CategoricalVariables_targetPlots(employee_data,\"EnvironmentSatisfaction\")\n",
    "\n",
    "we can see that people having low environment satisfaction 25% leave the company\n",
    "\n",
    "Gender Vs Attrition\n",
    "sns.boxplot(employee_data['Gender'], employee_data['MonthlyIncome'])\n",
    "plt.title('MonthlyIncome vs Gender Box Plot', fontsize=20)      \n",
    "plt.xlabel('MonthlyIncome', fontsize=16)\n",
    "plt.ylabel('Gender', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "CategoricalVariables_targetPlots(employee_data,\"Gender\")\n",
    "\n",
    "Monthly Income distribution for Male and Female is almost similar, so the attrition rate of Male and Female is almost the same around 15%. Gender is not a strong indicator of attrition\n",
    " \n",
    "fig,ax = plt.subplots(2,3, figsize=(20,20))               # 'ax' has references to all the four axes\n",
    "plt.suptitle(\"Comparision of various factors vs Gender\", fontsize=20)\n",
    "sns.barplot(employee_data['Gender'],employee_data['DistanceFromHome'],hue = employee_data['Attrition'], ax = ax[0,0],ci=None); \n",
    "sns.barplot(employee_data['Gender'],employee_data['YearsAtCompany'],hue = employee_data['Attrition'], ax = ax[0,1],ci=None); \n",
    "sns.barplot(employee_data['Gender'],employee_data['TotalWorkingYears'],hue = employee_data['Attrition'], ax = ax[0,2],ci=None); \n",
    "sns.barplot(employee_data['Gender'],employee_data['YearsInCurrentRole'],hue = employee_data['Attrition'], ax = ax[1,0],ci=None); \n",
    "sns.barplot(employee_data['Gender'],employee_data['YearsSinceLastPromotion'],hue = employee_data['Attrition'], ax = ax[1,1],ci=None); \n",
    "sns.barplot(employee_data['Gender'],employee_data['NumCompaniesWorked'],hue = employee_data['Attrition'], ax = ax[1,2],ci=None); \n",
    "plt.show()\n",
    "\n",
    "Distance from home matters to women employees more than men.\n",
    "Female employes are spending more years in one company compare to their counterpart.\n",
    "Female employes spending more years in current company are more inclined to switch.\n",
    " \n",
    "Job Role\n",
    "CategoricalVariables_targetPlots(employee_data,\"JobRole\",invert_axis=True)\n",
    "\n",
    "Jobs held by the employee is maximum in Sales Executive, then R&D , then Laboratory Technician\n",
    "People working in Sales department is most likely quit the company followed by Laboratory Technician and Human Resources there attrition rates are 40%, 24% and 22% respectively\n",
    "Marital Status\n",
    "CategoricalVariables_targetPlots(employee_data,\"MaritalStatus\")\n",
    "\n",
    "From the plot,it is understood that irrespective of the marital status,there are large people who stay with the company and do not leave.Therefore,marital status is a weak predictor of attrition\n",
    "\n",
    " \n",
    " \n",
    "Building Decision Tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#for fitting classification tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#to create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#import whole class of metrics\n",
    "from sklearn import metrics\n",
    "employee_data.Attrition.value_counts().plot(kind = \"bar\")\n",
    "plt.xlabel(\"Attrition\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "employee_data[\"Attrition\"].value_counts()\n",
    "No     1233\n",
    "Yes     237\n",
    "Name: Attrition, dtype: int64\n",
    " \n",
    "From the Exploratory data analysis, variable that are not significant to attrition are:\n",
    "\n",
    "EmployeeCount, EmployeeNumber, Gender, HourlyRate, JobLevel, MaritalStatus, Over18, StandardHours\n",
    "#copying the main employee data to another dataframe\n",
    "employee_data_new = employee_data.copy()\n",
    "#dropping the not significant variables\n",
    "employee_data_new.drop([\"EmployeeCount\",\"EmployeeNumber\",\"Gender\",\"HourlyRate\",\"Over18\",\"StandardHours\",\"left\"], axis=1,inplace=True)\n",
    " \n",
    "Handling Categorical Variables\n",
    "Segregate the numerical and Categorical variables\n",
    "Convert Categorical variables to dummy variables\n",
    "#data types of variables\n",
    "dict(employee_data_new.dtypes)\n",
    "{'Age': dtype('int64'),\n",
    " 'Attrition': dtype('O'),\n",
    " 'BusinessTravel': dtype('O'),\n",
    " 'DailyRate': dtype('int64'),\n",
    " 'Department': dtype('O'),\n",
    " 'DistanceFromHome': dtype('int64'),\n",
    " 'Education': dtype('int64'),\n",
    " 'EducationField': dtype('O'),\n",
    " 'EnvironmentSatisfaction': dtype('int64'),\n",
    " 'JobInvolvement': dtype('int64'),\n",
    " 'JobLevel': dtype('int64'),\n",
    " 'JobRole': dtype('O'),\n",
    " 'JobSatisfaction': dtype('int64'),\n",
    " 'MaritalStatus': dtype('O'),\n",
    " 'MonthlyIncome': dtype('int64'),\n",
    " 'MonthlyRate': dtype('int64'),\n",
    " 'NumCompaniesWorked': dtype('int64'),\n",
    " 'OverTime': dtype('O'),\n",
    " 'PercentSalaryHike': dtype('int64'),\n",
    " 'PerformanceRating': dtype('int64'),\n",
    " 'RelationshipSatisfaction': dtype('int64'),\n",
    " 'StockOptionLevel': dtype('int64'),\n",
    " 'TotalWorkingYears': dtype('int64'),\n",
    " 'TrainingTimesLastYear': dtype('int64'),\n",
    " 'WorkLifeBalance': dtype('int64'),\n",
    " 'YearsAtCompany': dtype('int64'),\n",
    " 'YearsInCurrentRole': dtype('int64'),\n",
    " 'YearsSinceLastPromotion': dtype('int64'),\n",
    " 'YearsWithCurrManager': dtype('int64')}\n",
    "#segregating the variables based on datatypes\n",
    "\n",
    "numeric_variable_names  = [key for key in dict(employee_data_new.dtypes) if dict(employee_data_new.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "categorical_variable_names = [key for key in dict(employee_data_new.dtypes) if dict(employee_data_new.dtypes)[key] in [\"object\"]]\n",
    "categorical_variable_names\n",
    "['Attrition',\n",
    " 'BusinessTravel',\n",
    " 'Department',\n",
    " 'EducationField',\n",
    " 'JobRole',\n",
    " 'MaritalStatus',\n",
    " 'OverTime']\n",
    "#store the numerical variables data in seperate dataset\n",
    "\n",
    "employee_data_num = employee_data_new[numeric_variable_names]\n",
    "#store the categorical variables data in seperate dataset\n",
    "\n",
    "employee_data_cat = employee_data_new[categorical_variable_names]\n",
    "#dropping the attrition \n",
    "employee_data_cat.drop([\"Attrition\"],axis=1,inplace=True)\n",
    "#converting into dummy variables\n",
    "\n",
    "employee_data_cat = pd.get_dummies(employee_data_cat)\n",
    "#Merging the both numerical and categorical data\n",
    "\n",
    "employee_data_final = pd.concat([employee_data_num, employee_data_cat,employee_data_new[[\"Attrition\"]]],axis=1)\n",
    "employee_data_final.head()\n",
    "Age\tDailyRate\tDistanceFromHome\tEducation\tEnvironmentSatisfaction\tJobInvolvement\tJobLevel\tJobSatisfaction\tMonthlyIncome\tMonthlyRate\t...\tJobRole_Research Director\tJobRole_Research Scientist\tJobRole_Sales Executive\tJobRole_Sales Representative\tMaritalStatus_Divorced\tMaritalStatus_Married\tMaritalStatus_Single\tOverTime_No\tOverTime_Yes\tAttrition\n",
    "0\t41\t1102\t1\t2\t2\t3\t2\t4\t5993\t19479\t...\t0\t0\t1\t0\t0\t0\t1\t0\t1\tYes\n",
    "1\t49\t279\t8\t1\t3\t2\t2\t2\t5130\t24907\t...\t0\t1\t0\t0\t0\t1\t0\t1\t0\tNo\n",
    "2\t37\t1373\t2\t2\t4\t2\t1\t3\t2090\t2396\t...\t0\t0\t0\t0\t0\t0\t1\t0\t1\tYes\n",
    "3\t33\t1392\t3\t4\t4\t3\t1\t3\t2909\t23159\t...\t0\t1\t0\t0\t0\t1\t0\t0\t1\tNo\n",
    "4\t27\t591\t2\t1\t1\t3\t1\t2\t3468\t16632\t...\t0\t0\t0\t0\t0\t1\t0\t1\t0\tNo\n",
    "5 rows × 49 columns\n",
    "\n",
    "#final features\n",
    "features =  list(employee_data_final.columns.difference([\"Attrition\"]))\n",
    "features\n",
    "['Age',\n",
    " 'BusinessTravel_Non-Travel',\n",
    " 'BusinessTravel_Travel_Frequently',\n",
    " 'BusinessTravel_Travel_Rarely',\n",
    " 'DailyRate',\n",
    " 'Department_Human Resources',\n",
    " 'Department_Research & Development',\n",
    " 'Department_Sales',\n",
    " 'DistanceFromHome',\n",
    " 'Education',\n",
    " 'EducationField_Human Resources',\n",
    " 'EducationField_Life Sciences',\n",
    " 'EducationField_Marketing',\n",
    " 'EducationField_Medical',\n",
    " 'EducationField_Other',\n",
    " 'EducationField_Technical Degree',\n",
    " 'EnvironmentSatisfaction',\n",
    " 'JobInvolvement',\n",
    " 'JobLevel',\n",
    " 'JobRole_Healthcare Representative',\n",
    " 'JobRole_Human Resources',\n",
    " 'JobRole_Laboratory Technician',\n",
    " 'JobRole_Manager',\n",
    " 'JobRole_Manufacturing Director',\n",
    " 'JobRole_Research Director',\n",
    " 'JobRole_Research Scientist',\n",
    " 'JobRole_Sales Executive',\n",
    " 'JobRole_Sales Representative',\n",
    " 'JobSatisfaction',\n",
    " 'MaritalStatus_Divorced',\n",
    " 'MaritalStatus_Married',\n",
    " 'MaritalStatus_Single',\n",
    " 'MonthlyIncome',\n",
    " 'MonthlyRate',\n",
    " 'NumCompaniesWorked',\n",
    " 'OverTime_No',\n",
    " 'OverTime_Yes',\n",
    " 'PercentSalaryHike',\n",
    " 'PerformanceRating',\n",
    " 'RelationshipSatisfaction',\n",
    " 'StockOptionLevel',\n",
    " 'TotalWorkingYears',\n",
    " 'TrainingTimesLastYear',\n",
    " 'WorkLifeBalance',\n",
    " 'YearsAtCompany',\n",
    " 'YearsInCurrentRole',\n",
    " 'YearsSinceLastPromotion',\n",
    " 'YearsWithCurrManager']\n",
    " \n",
    "Separating the Target and the Predictors\n",
    "#seperating the target and predictors\n",
    "\n",
    "X = employee_data_final[features]\n",
    "y = employee_data_final[[\"Attrition\"]]\n",
    "X.shape\n",
    "(1470, 48)\n",
    "Train-Test Split(Stratified Sampling of Y)\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#function for crossvalidate score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#to find the best \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size = 0.3,stratify = y,random_state = 100)\n",
    "#Checks\n",
    "#Proportion in training data\n",
    "y_train.Attrition.value_counts()/len(y_train)\n",
    "No     0.838678\n",
    "Yes    0.161322\n",
    "Name: Attrition, dtype: float64\n",
    "#Checks\n",
    "#Proportion in training data\n",
    "pd.DataFrame(y_train.Attrition.value_counts()/len(y_train)).plot(kind = \"bar\")\n",
    "plt.show()\n",
    "\n",
    "#Proportion of test data\n",
    "y_test.Attrition.value_counts()/len(y_test)\n",
    "No     0.839002\n",
    "Yes    0.160998\n",
    "Name: Attrition, dtype: float64\n",
    "#make a pipeline for decision tree model \n",
    "\n",
    "pipelines = {\n",
    "    \"clf\": make_pipeline(DecisionTreeClassifier(max_depth=3,random_state=100))\n",
    "}\n",
    "Cross Validate\n",
    "To check the accuracy of the pipeline\n",
    "scores = cross_validate(pipelines['clf'], X_train, y_train,return_train_score=True)\n",
    "scores['test_score'].mean()\n",
    "0.8338352836423178\n",
    "Average accuracy of pipeline with Decision Tree Classifier is 83.48%\n",
    "\n",
    "Cross-Validation and Hyper Parameters Tuning\n",
    "Cross Validation is the process of finding the best combination of parameters for the model by traning and evaluating the model for each combination of the parameters\n",
    "\n",
    "Declare a hyper-parameters to fine tune the Decision Tree Classifier\n",
    "Decision Tree is a greedy alogritum it searches the entire space of possible decision trees. so we need to find a optimum parameter(s) or criteria for stopping the decision tree at some point. We use the hyperparameters to prune the decision tree\n",
    "\n",
    "decisiontree_hyperparameters = {\n",
    "    \"decisiontreeclassifier__max_depth\": np.arange(3,12),\n",
    "    \"decisiontreeclassifier__max_features\": np.arange(3,10),\n",
    "    \"decisiontreeclassifier__min_samples_split\": [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "    \"decisiontreeclassifier__min_samples_leaf\" : np.arange(1,3)\n",
    "}\n",
    "pipelines['clf']\n",
    "Pipeline(memory=None,\n",
    "     steps=[('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
    "            splitter='best'))])\n",
    " \n",
    "Decision Tree classifier with gini index\n",
    "Fit and tune models with cross-validation\n",
    "Now that we have our pipelines and hyperparameters dictionaries declared, we're ready to tune our models with cross-validation.\n",
    "\n",
    "We are doing 5 fold cross validation\n",
    "#Create a cross validation object from decision tree classifier and it's hyperparameters\n",
    "\n",
    "clf_model = GridSearchCV(pipelines['clf'], decisiontree_hyperparameters, cv=5, n_jobs=-1)\n",
    "#fit the model with train data\n",
    "clf_model.fit(X_train, y_train)\n",
    "GridSearchCV(cv=5, error_score='raise',\n",
    "       estimator=Pipeline(memory=None,\n",
    "     steps=[('decisiontreeclassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
    "            splitter='best'))]),\n",
    "       fit_params=None, iid=True, n_jobs=-1,\n",
    "       param_grid={'decisiontreeclassifier__max_depth': array([ 3,  4,  5,  6,  7,  8,  9, 10, 11]), 'decisiontreeclassifier__max_features': array([3, 4, 5, 6, 7, 8, 9]), 'decisiontreeclassifier__min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'decisiontreeclassifier__min_samples_leaf': array([1, 2])},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=0)\n",
    " \n",
    "#Display the best parameters for Decision Tree Model\n",
    "clf_model.best_params_\n",
    "{'decisiontreeclassifier__max_depth': 3,\n",
    " 'decisiontreeclassifier__max_features': 7,\n",
    " 'decisiontreeclassifier__min_samples_leaf': 1,\n",
    " 'decisiontreeclassifier__min_samples_split': 10}\n",
    "#Display the best score for the fitted model\n",
    "clf_model.best_score_\n",
    "0.8561710398445093\n",
    "#In Pipeline we can use the string names to get the decisiontreeclassifer\n",
    "\n",
    "clf_model.best_estimator_.named_steps['decisiontreeclassifier']\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
    "            max_features=7, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "            min_impurity_split=None, min_samples_leaf=1,\n",
    "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=100, splitter='best')\n",
    "#saving into a variable to get graph\n",
    "\n",
    "clf_best_model = clf_model.best_estimator_.named_steps['decisiontreeclassifier']\n",
    " \n",
    "Model Performance Evaluation\n",
    "On Test Data\n",
    "#Making a dataframe of actual and predicted data from test set\n",
    "\n",
    "tree_test_pred = pd.concat([y_test.reset_index(drop = True),pd.DataFrame(clf_model.predict(X_test))],axis=1)\n",
    "tree_test_pred.columns = [\"actual\",\"predicted\"]\n",
    "\n",
    "#setting the index to original index\n",
    "tree_test_pred.index = y_test.index\n",
    "tree_test_pred.head()\n",
    "actual\tpredicted\n",
    "34\tYes\tYes\n",
    "1432\tNo\tNo\n",
    "334\tNo\tNo\n",
    "1068\tYes\tNo\n",
    "736\tNo\tNo\n",
    "#keeping only positive condition (yes for attrition)\n",
    "\n",
    "pred_probability = pd.DataFrame(p[1] for p in clf_model.predict_proba(X_test))\n",
    "pred_probability.columns = [\"predicted_prob\"]\n",
    "pred_probability.index = y_test.index\n",
    "#merging the predicted data and its probability value\n",
    "\n",
    "tree_test_pred = pd.concat([tree_test_pred,pred_probability],axis=1)\n",
    "tree_test_pred.head()\n",
    "actual\tpredicted\tpredicted_prob\n",
    "34\tYes\tYes\t0.632184\n",
    "1432\tNo\tNo\t0.220859\n",
    "334\tNo\tNo\t0.072165\n",
    "1068\tYes\tNo\t0.145985\n",
    "736\tNo\tNo\t0.072165\n",
    "#converting the labels Yes --> 1 and No --> 0 for further operations below\n",
    "\n",
    "tree_test_pred[\"actual_left\"] = np.where(tree_test_pred[\"actual\"] == \"Yes\",1,0)\n",
    "tree_test_pred[\"predicted_left\"] = np.where(tree_test_pred[\"predicted\"] == \"Yes\",1,0)\n",
    "tree_test_pred.head()\n",
    "actual\tpredicted\tpredicted_prob\tactual_left\tpredicted_left\n",
    "34\tYes\tYes\t0.632184\t1\t1\n",
    "1432\tNo\tNo\t0.220859\t0\t0\n",
    "334\tNo\tNo\t0.072165\t0\t0\n",
    "1068\tYes\tNo\t0.145985\t1\t0\n",
    "736\tNo\tNo\t0.072165\t0\t0\n",
    "Confusion Matrix\n",
    "The confusion matrix is a way of tabulating the number of misclassifications, i.e., the number of predicted classes which ended up in a wrong classification bin based on the true classes.\n",
    "\n",
    "#confusion matrix\n",
    "metrics.confusion_matrix(tree_test_pred.actual,tree_test_pred.predicted,labels=[\"Yes\",\"No\"])\n",
    "array([[ 21,  50],\n",
    "       [ 26, 344]], dtype=int64)\n",
    "#confusion matrix visualization using seaborn heatmap\n",
    "\n",
    "sns.heatmap(metrics.confusion_matrix(tree_test_pred.actual,tree_test_pred.predicted,\n",
    "                                    labels=[\"Yes\",\"No\"]),cmap=\"Greens\",annot=True,fmt=\".2f\",\n",
    "           xticklabels = [\"Left\", \"Not Left\"] , yticklabels = [\"Left\", \"Not Left\"])\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "#Area Under ROC Curve\n",
    "\n",
    "auc_score_test = metrics.roc_auc_score(tree_test_pred.actual_left,tree_test_pred.predicted_left)\n",
    "print(\"AUROC Score:\",round(auc_score_test,4))\n",
    "AUROC Score: 0.6128\n",
    "##Plotting the ROC Curve\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(tree_test_pred.actual_left, tree_test_pred.predicted_prob,drop_intermediate=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot( fpr, tpr, label='ROC curve (area = %0.4f)' % auc_score_test)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic cuve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "From the ROC Curve, we have a choice to make depending on the value we place on true positive and tolerance for false positive rate\n",
    "\n",
    "If we wish to find the more people who are leaving, we could increase the true positive rate by adjusting the probability cutoff for classification. However by doing so would also increase the false positive rate. we need to find the optimum value of cutoff for classification\n",
    "Metrics\n",
    "Recall: Ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized\n",
    "Precision: To get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive\n",
    "#calculating the recall score\n",
    "\n",
    "print(\"Recall Score:\",round(metrics.recall_score(tree_test_pred.actual_left,tree_test_pred.predicted_left) * 100,3))\n",
    "Recall Score: 29.577\n",
    "#calculating the precision score\n",
    "\n",
    "print(\"Precision Score:\",round(metrics.precision_score(tree_test_pred.actual_left,tree_test_pred.predicted_left) * 100,3))\n",
    "Precision Score: 44.681\n",
    "print(metrics.classification_report(tree_test_pred.actual_left,tree_test_pred.predicted_left))\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.87      0.93      0.90       370\n",
    "          1       0.45      0.30      0.36        71\n",
    "\n",
    "avg / total       0.80      0.83      0.81       441\n",
    "\n",
    "Visualization of Decision Tree\n",
    "Dependencies\n",
    "Need to install graphviz (conda install pydot graphviz)\n",
    "Set the environment path variable to graphviz folder\n",
    "# conda install pydot graphviz\n",
    "#! pip install pydotplus\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus as pdot\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus as pdot\n",
    "#import os     \n",
    "#os.environ[\"PATH\"] += os.pathsep + 'C:/Users/NiranjanKumar/Anaconda3/Library/bin/graphviz'\n",
    "#write the dot data\n",
    "dot_data = StringIO()\n",
    "#export the decision tree along with the feature names into a dot file format\n",
    "\n",
    "export_graphviz(clf_best_model,out_file=dot_data,filled=True,\n",
    "                rounded=True,special_characters=True,feature_names = X_train.columns.values,class_names = [\"No\",\"Yes\"])\n",
    "#make a graph from dot file \n",
    "graph = pdot.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())\n",
    "\n",
    "#export the tree diagram\n",
    "graph.write_png(\"employee_attirtion.png\")\n",
    "True\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
