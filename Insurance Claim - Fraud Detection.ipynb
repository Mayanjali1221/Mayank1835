{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f629f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import ipywidgets as widgets\n",
    "from scipy import special\n",
    "\n",
    "py.offline.init_notebook_mode(connected=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load & view raw data\n",
    "df = pd.read_csv('insurance_claims.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6941c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3f9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde52a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ff435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf395ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.value_counts(df['fraud_reported']).plot.bar(color=['blue', 'red'], figsize=(10,5))\n",
    "ax.set_xlabel('Fraud reported')\n",
    "ax.set_ylabel('Number of claims')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fraud_reported'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad31b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which states did the incidents occur\n",
    "df['incident_state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(df, x='incident_state', y='total_claim_amount', color='incident_state' )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93087988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# breakdown of the incident states as a function of incident dates\n",
    "fig = px.bar(df, x='incident_date', y='fraud_reported', color='incident_state' )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c481172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of age \n",
    "fig = px.histogram(df, x='age', color='fraud_reported', histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd03c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total claim amount distribution\n",
    "fig = px.histogram(df, x=\"total_claim_amount\", color=\"fraud_reported\", marginal=\"box\", # can be 'rug', `box`, `violin`\n",
    "                         hover_data=df.columns)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution in Gender\n",
    "labels = ['Male', 'Female']\n",
    "fig = px.pie(df, values=df['insured_sex'].value_counts(), names=labels, title='% Gender')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of the incidents\n",
    "fig = px.pie(df, values=df['incident_type'].value_counts(), names=df['incident_type'].value_counts().keys(), title='Incident Type', )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.auto_make =='Saab'].fraud_reported.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a4c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.auto_make =='BMW'].fraud_reported.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c53ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='auto_make',  color='fraud_reported')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61543d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='insured_education_level',  color='fraud_reported')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ef497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='insured_hobbies',  color='fraud_reported')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='incident_severity',  color='fraud_reported')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ec107",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df, values=df['incident_severity'].value_counts(), names=df['incident_severity'].value_counts().keys(), title='Incident Severity')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='incident_type',  color='fraud_reported')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b236a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='insured_education_level',  color='fraud_reported', histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4126caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='collision_type',  color='fraud_reported', histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cc44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(df, x='insured_occupation',  color='fraud_reported', histnorm='probability density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954bc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation and Modeling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "#from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "from pylab import rcParams\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "# Turning-off the warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b13cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load & view the data\n",
    "df = pd.read_csv('insurance_claims.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb3fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique entries. Useful to know the catagorical features\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of missing values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55fdd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column with missing values \n",
    "df.columns[df.isna().any()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with ? entries\n",
    "df.columns[(df == '?').any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.columns[(df == '?').any()]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15163440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['property_damage'].replace(to_replace='?', value='NO', inplace=True)\n",
    "df['police_report_available'].replace(to_replace='?', value='NO', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838082ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are duplicated entries\n",
    "df.duplicated(subset=None, keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b8010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping uninformative features\n",
    "colsToDelete = [\"policy_number\", \"policy_bind_date\", \"insured_zip\", \"incident_location\", \"incident_date\", \"_c39\"]\n",
    "df = df.drop(columns = colsToDelete, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.97\n",
    "# calculate correlations\n",
    "corr_matrix = df.corr().abs()\n",
    "# get the upper part of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# columns with correlation above threshold\n",
    "redundent = [column for column in upper.columns if any(upper[column] >= threshold)]\n",
    "print(f'Columns to drop with correlation > {threshold}: {redundent}')\n",
    "df.drop(columns=redundent, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = df._get_numeric_data().columns\n",
    "cat_features = list(set(df.columns) - set(num_features))\n",
    "cat_features.remove('fraud_reported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf45380",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[num_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f3863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cat_features].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the target column from the features\n",
    "y = df[\"fraud_reported\"].map({\"N\":0, \"Y\":1})\n",
    "X = df.drop(\"fraud_reported\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35886b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([(\"numerical\", \"passthrough\", num_features), \n",
    "                                  (\"categorical\", OneHotEncoder(sparse=False, handle_unknown=\"ignore\"),\n",
    "                                   cat_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", LogisticRegression(class_weight=\"balanced\", solver=\"liblinear\", random_state=42))])\n",
    "\n",
    "# Decision Tree\n",
    "dt_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", DecisionTreeClassifier(class_weight=\"balanced\"))])\n",
    "# LDA\n",
    "lda_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", LinearDiscriminantAnalysis())])\n",
    "\n",
    "# Random Forest\n",
    "rf_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                     (\"model\", RandomForestClassifier(class_weight=\"balanced\", n_estimators=100, n_jobs=-1))])\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = Pipeline([(\"preprocessor\", preprocessor), \n",
    "                      # Add a scale_pos_weight to make it balanced\n",
    "                      (\"model\", XGBClassifier(scale_pos_weight=(1 - y.mean()), n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25478b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to plot the ROC and PVR curves \n",
    "def plot_eval(testY, predY, auc):  \n",
    "    fpr, tpr, thresh = sklearn.metrics.roc_curve(testY, predY[:,1])\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %.2f)' %auc)\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')\n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    precision_rt, recall_rt, threshold_rt = sklearn.metrics.precision_recall_curve(testY, predY[:,1])\n",
    "    plt.plot(recall_rt, precision_rt, linewidth=5, label='Precision-Recall curve')\n",
    "    plt.title('Recall vs Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247444c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluate(model, X, y, grid_params, plot_eval_curves = False):\n",
    "    \"\"\"Prepares a training and test set and evaluates the ML model\n",
    "       on multiple metrices \n",
    "    \n",
    "    Arguments:\n",
    "    ---------\n",
    "    model:        a defined ML model\n",
    "\n",
    "    X:            the feature matix\n",
    "\n",
    "    y:            the labels \n",
    "\n",
    "    grid_params:  hyperparameters to perform grid search on (dict)\n",
    "\n",
    "    plot_eval_curves: If False, outputs metrices \n",
    "                      If True, plots ROC and precision vs. recall curves \n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.2, random_state=555)\n",
    "    \n",
    "    gs = GridSearchCV(model, grid_params, \n",
    "                  n_jobs=-1, cv=5, scoring=\"roc_auc\")\n",
    "\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    model.set_params(**gs.best_params_)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities and labels\n",
    "    probs = model.predict_proba(X_test)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC AUC\n",
    "    auc = sklearn.metrics.roc_auc_score(y_test, probs[:, 1])\n",
    "    # get the confusion matrix\n",
    "    cnf_matrix = sklearn.metrics.confusion_matrix(y_test, preds)    \n",
    "    # Plot ROC curve\n",
    "    if plot_eval_curves:\n",
    "        plot_eval(y_test, probs, auc)\n",
    "    else: \n",
    "        print('Best Parameters:', gs.best_params_)\n",
    "        print('Best Score:', gs.best_score_)\n",
    "        print(f'ROC AUC: {round(auc, 4)}')\n",
    "        print(f'Confusion Matrix:\\n {cnf_matrix}')\n",
    "        # compute the other evaluation metrices \n",
    "        for metric in [sklearn.metrics.precision_score, sklearn.metrics.recall_score, sklearn.metrics.f1_score]:\n",
    "            print(f'{metric.__name__}: {round(metric(y_test, preds), 4)}')\n",
    "\n",
    "        # Average performance using 5 x cross-validation    \n",
    "        score = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n",
    "        print('Cross-validation AUC score: ', score.mean()) \n",
    "    return model, cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203eae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid-search hyper Parameters to consider\n",
    "model_grid_params = {'lr_model': {\"model__C\": [1, 1.3, 1.5]}, 'dt_model': {\"model__max_depth\": [3, 5, 7], \"model__min_samples_split\": [2, 5]}, \n",
    "              'rf_model': {\"model__max_depth\": [20, 10, 15],\"model__min_samples_split\": [5, 10]}, 'lda_model': {}, \n",
    "               'xgb_model':{\"model__max_depth\": [5, 10], \"model__min_child_weight\": [5, 10]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cebcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lr_model, dt_model, rf_model, lda_model, xgb_model]\n",
    "model_keys = [('lr_model', 'Logistic Regression'),\n",
    "              ('dt_model', 'Decision Tree'),\n",
    "              ('rf_model', 'Random Forest'),\n",
    "              ('lda_model', 'Linear Discriminant Analysis'),\n",
    "              ('xgb_model', 'Gradient Boosting')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model, cnf_matrix = {}, {}\n",
    "for idx, model in enumerate(models):\n",
    "    print(45*'_', '\\n{}'.format(model_keys[idx][1]))\n",
    "    model, cnf = model_evaluate(model, X, y, model_grid_params[model_keys[idx][0]])\n",
    "    final_model[model_keys[idx][0]] = model\n",
    "    cnf_matrix[model_keys[idx][0]] = cnf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037a23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a17e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of the LDA model\n",
    "plot_confusion_matrix(cnf_matrix['lda_model'], target_names=['legitimate', 'fraud'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e7fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of the DT model\n",
    "plot_confusion_matrix(cnf_matrix['dt_model'], target_names=['legitimate', 'fraud'], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcb1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve LDA \n",
    "model, cnf = model_evaluate(lda_model, X, y, model_grid_params['lda_model'], plot_eval_curves=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a667a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision vs. Recall curves of the LDA model \n",
    "model, cnf = model_evaluate(lda_model, X, y, model_grid_params['lda_model'], plot_eval_curves=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0fdb33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
